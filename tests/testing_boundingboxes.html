<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Seekable Live Stream with Bounding Boxes</title>
  <style>
    body {
      margin: 0;
      overflow: hidden;
    }
    #videoWrapper {
      position: relative;
      width: 100%;
      max-width: 1280px;
      margin: auto;
    }
    video {
      width: 100%;
      height: auto;
      display: block;
    }
    canvas {
      position: absolute;
      top: 0;
      left: 0;
      pointer-events: none;
    }
  </style>
</head>
<body>

<div id="videoWrapper">
  <video id="video" controls muted playsinline></video>
  <canvas id="overlay"></canvas>
</div>

<script src="https://cdn.jsdelivr.net/npm/hls.js@latest"></script>
<script>
  const video = document.getElementById('video');
  const canvas = document.getElementById('overlay');
  const ctx = canvas.getContext('2d');

  const videoUrl = 'http://10.23.71.78:5090/media/2025-06-10/cam1/index.m3u8';
  const jsonBaseUrl = 'http://10.23.71.78:5090/media/2025-06-10/cam1/';
  
  const segmentDuration = 2; // seconds, should match backend SEGMENT_DURATION
  let currentSegmentIndex = -1;
  let currentMetadata = null;
  let metadataCache = {};

  // Setup HLS video playback
  function setupVideo() {
    if (Hls.isSupported()) {
      const hls = new Hls();
      hls.loadSource(videoUrl);
      hls.attachMedia(video);
      hls.on(Hls.Events.MANIFEST_PARSED, () => {
        video.play();
      });
    } else if (video.canPlayType('application/vnd.apple.mpegurl')) {
      video.src = videoUrl;
      video.play();
    }
  }

  // Resize canvas to match video dimensions
  function resizeCanvas() {
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
  }

  video.addEventListener('loadeddata', resizeCanvas);
  window.addEventListener('resize', resizeCanvas);

  // Get segment index from current video time
  function getSegmentIndexFromTime(timeInSeconds) {
    return Math.floor(timeInSeconds / segmentDuration);
  }

  // Fetch JSON metadata
  async function fetchMetadata(segmentIndex) {
    if (metadataCache[segmentIndex]) {
      return metadataCache[segmentIndex];
    }
    const jsonUrl = `${jsonBaseUrl}segment_${String(segmentIndex).padStart(5, '0')}.json`;
    try {
      const res = await fetch(jsonUrl);
      if (!res.ok) throw new Error("No segment yet");
      const metadata = await res.json();
      metadataCache[segmentIndex] = metadata;
      return metadata;
    } catch (err) {
      console.log('No metadata for segment:', segmentIndex);
      return null;
    }
  }

  // Draw bounding boxes on the canvas
  function drawBoundingBoxes(detections) {
    ctx.clearRect(0, 0, canvas.width, canvas.height);
    if (!detections) return;
    ctx.strokeStyle = 'lime';
    ctx.lineWidth = 2;
    ctx.font = '16px Arial';
    ctx.fillStyle = 'lime';
    for (const obj of detections) {
      const { xmin, ymin, xmax, ymax, label } = obj;
      ctx.strokeRect(xmin, ymin, xmax - xmin, ymax - ymin);
      ctx.fillText(label || 'object', xmin + 4, ymin - 6);
    }
  }

  // Update bounding boxes based on current video time
  async function updateBoundingBoxes() {
    const currentTime = video.currentTime;
    const segmentIndex = getSegmentIndexFromTime(currentTime);
    const timeWithinSegment = currentTime % segmentDuration;

    if (segmentIndex !== currentSegmentIndex) {
      currentSegmentIndex = segmentIndex;
      currentMetadata = await fetchMetadata(segmentIndex);
    }

    if (currentMetadata && currentMetadata.length > 0) {
      const num_frames = currentMetadata.length;
      const frame_index = Math.min(
        Math.max(0, Math.round((timeWithinSegment / segmentDuration) * num_frames)),
        num_frames - 1
      );
      const detections = currentMetadata[frame_index].detections;
      drawBoundingBoxes(detections);
    } else {
      ctx.clearRect(0, 0, canvas.width, canvas.height);
    }
  }

  setupVideo();
  video.addEventListener('timeupdate', updateBoundingBoxes);
</script>

</body>
</html>